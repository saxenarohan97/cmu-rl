from os import stat
import numpy as np

## TensorFlow Only ##
# import tensorflow as tf
# import keras
# from model_tensorflow import make_model


## Pytorch Only ##
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from model_pytorch import make_model, ExpertModel


def action_to_one_hot(env, action):
    action_vec = np.zeros(env.action_space.n)
    action_vec[action] = 1
    return action_vec    

def generate_episode(env, policy):
    """Collects one rollout from the policy in an environment. The environment
    should implement the OpenAI Gym interface. A rollout ends when done=True. The
    number of states and actions should be the same, so you should not include
    the final state when done=True.

    Args:
    env: an OpenAI Gym environment.
    policy: The output of a deep neural network
    Returns:
    states: a list of states visited by the agent.
    actions: a list of actions taken by the agent. For tensorflow, it will be 
        helpful to use a one-hot encoding to represent discrete actions. The actions 
        that you return should be one-hot vectors (use action_to_one_hot()).
        For Pytorch, the Cross-Entropy Loss function will integers for action
        labels.
    rewards: the reward received by the agent at each step.
    """
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    done = False
    state = env.reset()

    states = []
    actions = []
    rewards = []
    while not done:
        # WRITE CODE HERE
        states.append(state)
        with torch.no_grad():
            action = np.argmax(policy(torch.Tensor(state).to(device)).detach().cpu().numpy())
        actions.append(action)
        state, reward, done, _ = env.step(action)
        rewards.append(reward)
    
    return states, actions, rewards

class Imitation():
    
    def __init__(self, env, num_episodes, expert_file):
        self.env = env
        
        # TensorFlow Only #
        # self.expert = tf.keras.models.load_model(expert_file)
        
        # Pytorch Only #
        self.expert = ExpertModel()
        self.expert.load_state_dict(torch.load(expert_file))
        self.expert.eval()
        
        self.num_episodes = num_episodes
        
        self.model = make_model()
        
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.expert.to(self.device)
        self.model.to(self.device)

        self.loss = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters())

        self._train_states = None
        self._train_actions = None

    def generate_behavior_cloning_data(self):
        self._train_states = []
        self._train_actions = []
        for _ in range(self.num_episodes):
            states, actions, _ = generate_episode(self.env, self.expert)
            self._train_states.extend(states)
            self._train_actions.extend(actions)
        self._train_states = np.array(self._train_states)
        self._train_actions = np.array(self._train_actions)
        
    def generate_dagger_data(self):
        # WRITE CODE HERE
        # You should collect states and actions from the student policy
        # (self.model), and then relabel the actions using the expert policy.
        # This method does not return anything.
        # END

        new_train_states = []
        new_train_actions = []
        for _ in range(self.num_episodes):
            states, _, _ = generate_episode(self.env, self.model)
            actions = [np.argmax(self.expert(torch.Tensor(state).to(self.device)).detach().cpu().numpy())
                        for state in states]
            new_train_states.extend(states)
            new_train_actions.extend(actions)

        if self._train_states is None:
            self._train_states = np.array(new_train_states)
            self._train_actions = np.array(new_train_actions)
        else:
            self._train_states = np.r_[self._train_states, new_train_states]
            self._train_actions = np.r_[self._train_actions, new_train_actions]
    
    def train(self, num_epochs=1, batch_size=64):
        """
        Train the model on data generated by the expert policy.
        Use Cross-Entropy Loss and a batch size of 64 when
        performing updates.
        Args:
            num_epochs: number of epochs to train on the data generated by the expert.
        Return:
            loss: (float) final loss of the trained policy.
            acc: (float) final accuracy of the trained policy
        """
        # WRITE CODE HERE

        tensor_x = torch.Tensor(self._train_states)
        tensor_y = torch.Tensor(self._train_actions)

        trainset = torch.utils.data.TensorDataset(tensor_x, tensor_y)
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                                  shuffle=True, num_workers=2)

        for epoch in range(num_epochs):
            for data in trainloader:
                inputs, labels = data
                inputs = inputs.to(self.device)
                labels = labels.long().to(self.device)
                # zero the parameter gradients
                self.optimizer.zero_grad()
                # forward + backward + optimize
                outputs = self.model(inputs)
                loss = self.loss(outputs, labels)
                loss.backward()
                self.optimizer.step()

        # END

        total = 0
        correct = 0
        with torch.no_grad():
            for data in trainloader:
                states, labels = data
                states = states.to(self.device)
                labels = labels.long().to(self.device)
                # calculate outputs by running images through the network
                outputs = self.model(states)
                # the class with the highest energy is what we choose as prediction
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        acc = 100 * correct / total
        return loss.detach().cpu(), acc


    def evaluate(self, policy, n_episodes=50):
        rewards = []
        for i in range(n_episodes):
            _, _, r = generate_episode(self.env, policy)
            rewards.append(sum(r))
        r_mean = np.mean(rewards)
        return r_mean
    

# import gym
# env = gym.make('CartPole-v0')
# expert_file = 'expert_torch.pt'
# im = Imitation(env, 1, expert_file)
# im.generate_dagger_data()
# im.generate_dagger_data()
# im.generate_dagger_data()
# print(im._train_states.shape)
# print(im._train_actions.shape)
